{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils#; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"data/fish/sample/\"\n",
    "path = \"data/nof_excl/\"\n",
    "batch_size=32\n",
    "target_size = (360, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_val_feat = load_array(path+'results/conv_val_640.dat')\n",
    "# conv_trn_feat = load_array(path+'results/conv_trn_640.dat')\n",
    "# conv_test_feat = load_array(path+'results/conv_test_640.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model that represents common parts of VGG \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1282: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1282: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1282: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 360, 640) (None, 512, 22, 40)\n"
     ]
    }
   ],
   "source": [
    "vgg640 = Vgg16BN((360, 640)).model\n",
    "vgg640.pop()\n",
    "print (vgg640.input_shape, vgg640.output_shape)\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few convnet layers on top of vgg conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.\n",
    "\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "def get_additional_layers(conv_layers):\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(7,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "#         GlobalMaxPooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n"
     ]
    }
   ],
   "source": [
    "conv_layers,_ = split_at(vgg640, Convolution2D)\n",
    "model = Sequential(get_additional_layers(conv_layers))\n",
    "model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, zoom_range=0.05,\n",
    "                               height_shift_range=0.05, shear_range=0.05,\n",
    "                                      horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_flow = gen.flow_from_directory(path+\"train\", target_size=(360, 640),\n",
    "            class_mode=None, shuffle=False, batch_size=1)\n",
    "train_batches = np.concatenate([train_flow.next() for i in range(train_flow.nb_sample)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2912/2912 [==============================] - 361s   \n"
     ]
    }
   ],
   "source": [
    "conv_trn_feat = vgg640.predict(train_batches, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2912 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 14s - loss: 0.7993 - acc: 0.7284 - val_loss: 1.2626 - val_acc: 0.6800\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 14s - loss: 0.1786 - acc: 0.9481 - val_loss: 1.0820 - val_acc: 0.7400\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 14s - loss: 0.0521 - acc: 0.9870 - val_loss: 0.3246 - val_acc: 0.9175\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 14s - loss: 0.0196 - acc: 0.9962 - val_loss: 0.3571 - val_acc: 0.9175\n",
      "Epoch 5/5\n",
      "2912/2912 [==============================] - 14s - loss: 0.0050 - acc: 1.0000 - val_loss: 0.3277 - val_acc: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe86d485810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, nb_epoch=1,\n",
    "#             validation_data=val_batches, nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "# vgg640\n",
    "\n",
    "model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=5, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.optimizer.lr = 1e-4\n",
    "\n",
    "# for i in range(3):\n",
    "#     #calculate conv features for the new batch\n",
    "#     train_flow = gen.flow_from_directory(path+\"train\", target_size=(360, 640),\n",
    "#             class_mode=None, shuffle=False, batch_size=1)\n",
    "#     train_batches = np.concatenate([train_flow.next() for i in range(train_flow.nb_sample)])\n",
    "#     conv_trn_feat = vgg640.predict(train_batches, batch_size=32, verbose=1)\n",
    "    \n",
    "#     #fit model with small learning rate on new batch\n",
    "#     model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=10, \n",
    "#              validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An end-to-end convnet without pre-computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, zoom_range=0.05,\n",
    "                                      channel_shift_range=10, height_shift_range=0.1, shear_range=0.05,\n",
    "                                      fill_mode='nearest',horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_flow = gen.flow_from_directory(path+\"train\", target_size=target_size,\n",
    "            class_mode=\"categorical\", shuffle=True, batch_size=batch_size)\n",
    "\n",
    "val_flow = image.ImageDataGenerator().flow_from_directory(path+\"valid\", target_size=target_size, batch_size=batch_size,\n",
    "                                                   class_mode='categorical', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.\n",
    "\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "def get_additional_layers(conv_layers):\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(7,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "#         GlobalMaxPooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "(None, 3, 360, 640) (None, 7)\n"
     ]
    }
   ],
   "source": [
    "model = Vgg16BN((360, 640)).model\n",
    "model.pop()\n",
    "\n",
    "for layer in model.layers: #Freeze feature layers\n",
    "    layer.trainable = False\n",
    "    \n",
    "    \n",
    "    \n",
    "conv_layers,_ = split_at(model, Convolution2D)\n",
    "\n",
    "for l in get_additional_layers(conv_layers):\n",
    "    model.add(l)\n",
    "    \n",
    "print (model.input_shape, model.output_shape)\n",
    "model.compile(Adam(lr=0.001), 'categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_additional_layers(conv_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 436s - loss: 0.0712 - acc: 0.9763 - val_loss: 0.2529 - val_acc: 0.9150\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 436s - loss: 0.0822 - acc: 0.9742 - val_loss: 0.2888 - val_acc: 0.9400\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 436s - loss: 0.0763 - acc: 0.9739 - val_loss: 0.2587 - val_acc: 0.9525\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 436s - loss: 0.0817 - acc: 0.9718 - val_loss: 0.1914 - val_acc: 0.9575\n",
      "Epoch 5/5\n",
      "2912/2912 [==============================] - 436s - loss: 0.0578 - acc: 0.9818 - val_loss: 0.2141 - val_acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f014e409ed0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save_weights(path+\"results/with_aug_after_10epochs_val_acc_0.96.hd5\")\n",
    "# model.load_weights(path+\"results/with_aug_after_10epochs_val_acc_0.96.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 438s - loss: 0.0474 - acc: 0.9839 - val_loss: 0.1800 - val_acc: 0.9625\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 439s - loss: 0.0302 - acc: 0.9907 - val_loss: 0.1646 - val_acc: 0.9650\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 439s - loss: 0.0240 - acc: 0.9928 - val_loss: 0.1805 - val_acc: 0.9675\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 439s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.1725 - val_acc: 0.9675\n",
      "Epoch 5/5\n",
      "2912/2912 [==============================] - 439s - loss: 0.0157 - acc: 0.9959 - val_loss: 0.1827 - val_acc: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd236e1e4d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save_weights(path+\"models/with_aug_after_5epochs_e_4_val_acc_9675.hd5\")\n",
    "model.load_weights(path+\"models/with_aug_after_5epochs_e_4_val_acc_9675.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the original top 2 layers trainable\n",
    "After getting good enough accuracy on default VGG weights, it's time to allow the two top layers of original VGG to adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.layers\n",
    "#model.layers.trainable = true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction round 1\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round 2\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round 3\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round 4\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round 5\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = gen\n",
    "num_iterations = 5\n",
    "preds = np.zeros((len(test_filenames), 7)) #7 fish types \n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print (\"Starting prediction round {}\".format(i+1))\n",
    "    \n",
    "    test_gen = test_datagen.flow_from_directory(path+\"test\", target_size=target_size, batch_size=32,\n",
    "                                                        class_mode=None, shuffle=False)\n",
    "    preds += model.predict_generator(test_gen, val_samples=len(test_filenames))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.1142e+00,   5.5797e-04,   1.5526e-03,   5.0141e-04,   1.2738e-03,   1.2471e-05,\n",
       "          8.8194e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averaged_preds = preds / num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12153, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_array(path+\"results/preds_with_train_n_test_augmentation_val_acc_0.9675.dat\",averaged_preds)\n",
    "save_array(path+\"results/stg2_preds_with_train_n_test_augmentation_val_acc_0.9675.dat\",averaged_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averaged_preds = load_array(path+\"results/stg2_preds_with_train_n_test_augmentation_val_acc_0.9675.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer on Top\n",
    "\n",
    "This will serve as the second model for bagging.\n",
    "Very light augmentation for this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 360, 640) (None, 512, 22, 40)\n"
     ]
    }
   ],
   "source": [
    "from vgg16bn import Vgg16BN\n",
    "\n",
    "vgg640 = Vgg16BN(size=(360, 640)).model\n",
    "\n",
    "vgg640.pop()\n",
    "print (vgg640.input_shape, vgg640.output_shape)\n",
    "vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "conv_layers,fc_layers = split_at(vgg640, Convolution2D)\n",
    "conv_model = Sequential(conv_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trn = load_array(path+'results/trn_640.dat')\n",
    "# val = load_array(path+'results/val_640.dat')\n",
    "# # test = load_array(path+'results/val_640.dat')\n",
    "\n",
    "# conv_feat = conv_model.predict(trn)\n",
    "# conv_val_feat = conv_model.predict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_val_feat = load_array(path+'results/conv_val_640.dat')\n",
    "# conv_trn_feat = load_array(path+'results/conv_trn_640.dat')\n",
    "# conv_test_feat = load_array(path+'results/conv_test_640.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = image.ImageDataGenerator(\n",
    "        shear_range=0.05,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=7.,\n",
    "        width_shift_range=0.07,\n",
    "        height_shift_range=0.07,\n",
    "    channel_shift_range=5,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "# set shuffle=False if pre-\n",
    "\n",
    "# aug_trn_batches = train_datagen.flow_from_directory(path+'train', target_size=target_size,\n",
    "#            class_mode='categorical', shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# aug_val_batches = image.ImageDataGenerator().flow_from_directory(path+\"valid\", target_size=target_size, batch_size=batch_size,\n",
    "#                                                    class_mode='categorical', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# da_conv_featx2 = vgg640.predict_generator(aug_trn_batches, aug_trn_batches.nb_sample*2)\n",
    "# save_array(path+\"results/da_conv_featx2.dat\",da_conv_featx2)\n",
    "da_conv_featx2 = load_array(path+\"results/da_conv_featx2.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv_trn_feat = load_array(path+'results/conv_trn_640.dat')\n",
    "# conv_val_feat = load_array(path+'results/conv_val_640.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's include the real training data as well in its non-augmented form.\n",
    "da_conv_feat = np.concatenate([da_conv_featx2, conv_trn_feat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Since we've now got a dataset 6x bigger than before, we'll need to copy our labels 6 times too.\n",
    "da_trn_labels = np.concatenate([trn_labels]*3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# save_array(path+'results/conv_val_feat_640.dat', conv_val_feat)\n",
    "# save_array(path+'results/conv_feat_640.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/3),\n",
    "        Flatten(),\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(7, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 0.6\n",
    "# bn_model = vgg_ft_bn(1)\n",
    "# conv_layers,fc_layers = split_at(vgg640, Convolution2D)\n",
    "\n",
    "dense_model = Sequential(get_bn_layers(p))\n",
    "dense_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8736 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "8736/8736 [==============================] - 63s - loss: 1.1111 - acc: 0.7491 - val_loss: 0.1354 - val_acc: 0.9750\n",
      "Epoch 2/5\n",
      "8736/8736 [==============================] - 63s - loss: 0.2143 - acc: 0.9439 - val_loss: 0.1067 - val_acc: 0.9725\n",
      "Epoch 3/5\n",
      "8736/8736 [==============================] - 63s - loss: 0.1318 - acc: 0.9669 - val_loss: 0.1335 - val_acc: 0.9750\n",
      "Epoch 4/5\n",
      "8736/8736 [==============================] - 63s - loss: 0.0821 - acc: 0.9783 - val_loss: 0.1334 - val_acc: 0.9750\n",
      "Epoch 5/5\n",
      "8736/8736 [==============================] - 63s - loss: 0.0655 - acc: 0.9844 - val_loss: 0.1206 - val_acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63f3c9fb50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dense_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=5, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dense_model.save_weights(path+\"models/dense_augx2_1e3_5epoc_valacc_0.9750.h5\")\n",
    "dense_model.load_weights(path+\"models/dense_augx2_1e3_5epoc_valacc_0.9750.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8736 samples, validate on 400 samples\n",
      "Epoch 1/2\n",
      "8736/8736 [==============================] - 63s - loss: 0.0352 - acc: 0.9910 - val_loss: 0.1238 - val_acc: 0.9750\n",
      "Epoch 2/2\n",
      "8736/8736 [==============================] - 63s - loss: 0.0384 - acc: 0.9894 - val_loss: 0.1188 - val_acc: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1146c10ad0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.optimizer.lr = 1e-6\n",
    "\n",
    "dense_model.fit(da_conv_feat, da_trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dense_model.save_weights(path+\"models/dense_augx2_1e6_2epoc_valacc_0.9800.h5\")\n",
    "dense_model.load_weights(path+\"models/dense_augx2_1e6_2epoc_valacc_0.9800.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cleanup\n",
    "# del da_conv_feat, conv_val_feat, da_conv_featx2, conv_trn_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction round  1\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-2.7.12-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction round  2\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round  3\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round  4\n",
      "Found 12153 images belonging to 1 classes.\n",
      "Starting prediction round  5\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros(shape=(len(test_filenames), 7))\n",
    "\n",
    "nb_aug = 5\n",
    "for aug in range(nb_aug):\n",
    "    print (\"Starting prediction round \", aug+1)\n",
    "\n",
    "    test_batches = train_datagen.flow_from_directory(path+\"test\", target_size=target_size,\n",
    "                class_mode=None, shuffle=False, batch_size=32)\n",
    "\n",
    "\n",
    "    conv_test_feat = vgg640.predict_generator(test_batches, len(test_filenames))\n",
    "    predictions += dense_model.predict(conv_test_feat, batch_size=32)\n",
    "    \n",
    "    del conv_test_feat # release memory to avoid memory exception\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.7667e+00,   1.6435e-02,   1.1405e-02,   1.0580e-03,   3.6808e-02,   6.1723e-04,\n",
       "          1.6694e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions /= nb_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output from full convnet for comparison: \n",
    "\n",
    "array([[  4.1142e+00,   5.5797e-04,   1.5526e-03,   5.0141e-04,   1.2738e-03,   1.2471e-05,\n",
    "          8.8194e-01]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+\"results/stg2_preds_dense_augx2_1e6_valacc_0.9800.dat\",predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = load_array(path+\"results/stg2_preds_dense_augx2_1e6_valacc_0.9800.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.7667e+00,   1.6435e-02,   1.1405e-02,   1.0580e-03,   3.6808e-02,   6.1723e-04,\n",
       "         1.6694e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MixIterator(object):\n",
    "    def __init__(self, iters):\n",
    "        self.iters = iters\n",
    "        self.multi = type(iters) is list\n",
    "#         if self.multi:\n",
    "#             self.N = sum([it[0].N for it in self.iters])\n",
    "#         else:\n",
    "        self.N = sum([it.N for it in self.iters])\n",
    "\n",
    "    def reset(self):\n",
    "        for it in self.iters: it.reset()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self, *args, **kwargs):\n",
    "#         if self.multi:\n",
    "#             nexts = [[next(it) for it in o] for o in self.iters]\n",
    "#             n0s = np.concatenate([n[0] for n in o])\n",
    "#             n1s = np.concatenate([n[1] for n in o])\n",
    "#             return (n0, n1)\n",
    "#         else:\n",
    "        nexts = [next(it) for it in self.iters]\n",
    "        n0 = np.concatenate([n[0] for n in nexts])\n",
    "        n1 = np.concatenate([n[1] for n in nexts])\n",
    "        return (n0, n1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pseudo = dense_model.predict(conv_val_feat, batch_size=batch_size)\n",
    "test_pseudo = dense_model.predict(conv_test_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "\n",
    "\n",
    "trn_batch  = gen.flow(da_conv_feat, da_trn_labels,batch_size=44)\n",
    "val_batch  = gen.flow(conv_val_feat,val_pseudo,batch_size=4)\n",
    "test_batch = gen.flow(conv_test_feat,test_pseudo,batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mi = MixIterator([trn_batch, val_batch,test_batch])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7224/7224 [==============================] - 230s - loss: 0.1263 - acc: 0.9695 - val_loss: 0.0857 - val_acc: 0.9775\n",
      "Epoch 2/8\n",
      "7252/7224 [==============================] - 230s - loss: 0.1206 - acc: 0.9732 - val_loss: 0.0872 - val_acc: 0.9775\n",
      "Epoch 3/8\n",
      "7252/7224 [==============================] - 230s - loss: 0.1197 - acc: 0.9750 - val_loss: 0.0898 - val_acc: 0.9775\n",
      "Epoch 4/8\n",
      " 256/7224 [>.............................] - ETA: 206s - loss: 0.1249 - acc: 0.9648"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-044f6d31187b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdense_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdense_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_val_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1409\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dense_model.optimizer.lr = 1e-3\n",
    "dense_model.fit_generator(mi, mi.N, nb_epoch=8, validation_data=(conv_val_feat, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7252/7224 [==============================] - 235s - loss: 0.1130 - acc: 0.9756 - val_loss: 0.0925 - val_acc: 0.9750\n",
      "Epoch 2/5\n",
      "7252/7224 [==============================] - 231s - loss: 0.1070 - acc: 0.9793 - val_loss: 0.0933 - val_acc: 0.9750\n",
      "Epoch 3/5\n",
      "7252/7224 [==============================] - 231s - loss: 0.1098 - acc: 0.9790 - val_loss: 0.0949 - val_acc: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff48d9f1d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.optimizer.lr = 1e-5\n",
    "dense_model.fit_generator(mi, mi.N, nb_epoch=5, validation_data=(conv_val_feat, val_labels), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_model.save_weights(path+\"models/dense_psudeo_augx1_1e4_13epoc_valacc_0.9775.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#old code, paths are inconsistent !ignore\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG','NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "# nofish_prob = load_array(\"data/fishnofish/results/preds_640_loss_0.0244_acc_0.9913_val_loss_0.2086_val_acc_0.9778.dat\")\n",
    "# nofish_prob = load_array(\"data/fishnofish/results/pred_stg1_fullconv_aug_no_pseudo.dat\")\n",
    "# nofish_prob = load_array(\"data/fishnofish/results/final_stg1_weights_ensumble.dat\")\n",
    "\n",
    "nofish_prob = load_array(\"data/fishnofish/results/final_stg1_weights_ensumble_with_pseudo.dat\")\n",
    "\n",
    "\n",
    "\n",
    "nofish_prob = nofish_prob[:, 1] #nofish class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stg1\n",
    "preds = load_array(path+\"results/preds_light_aug_cropped_v1.dat\")\n",
    "# preds_denseaug = load_array(path+\"results/preds_dense_precomputed_aug_1e7_val_acc_0.9625.dat\")\n",
    "\n",
    "#stg 2\n",
    "preds_denseaug = load_array(path+\"results/stg2_preds_dense_precomputed_aug_1e7_val_acc_0.9625.dat\")\n",
    "\"results/stg2_preds_dense_augx2_1e6_valacc_0.9800.dat\"\n",
    "\n",
    "subm_name = path+'submit/preds_cropped_v1_denseaugprecompute.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds+= preds_denseaug\n",
    "preds /= 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_predictions(predictions):\n",
    "    \"\"\"Weights predictions based on probability image contains a fish as predicted by fish detector model\"\"\"\n",
    "    no_fish = predictions[:, 4]\n",
    "    fish = np.delete(predictions, 4, axis=1)\n",
    "\n",
    "    weights = -1. * (no_fish - 1.)\n",
    "    weights = weights.reshape(1000, 1)\n",
    "\n",
    "    fish = weights * fish\n",
    "    preds = np.insert(fish, 4, no_fish, axis=1)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_nofish_probs = np.insert(preds, 4, nofish_prob, axis=1)\n",
    "weighted_preds = weight_predictions(with_nofish_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 0.85#0.82\n",
    "subm = np.clip(weighted_preds, (1-f)/7, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
