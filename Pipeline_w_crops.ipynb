{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils#; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"data/fish/sample/\"\n",
    "path = \"data/cropped_nof_excl/\"\n",
    "batch_size=32\n",
    "target_size = (450, 450)\n",
    "\n",
    "early_stopping  = keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n",
      "Found 12153 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_test_filenames = [f.split('/')[-1] for f in test_filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  End-to-end convnet without pre-computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, zoom_range=0.05,\n",
    "                                      channel_shift_range=5, height_shift_range=0.05, shear_range=0.05,\n",
    "                                      fill_mode='nearest',horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_flow = gen.flow_from_directory(path+\"train\", target_size=target_size,\n",
    "            class_mode=\"categorical\", shuffle=True, batch_size=batch_size)\n",
    "\n",
    "val_flow = image.ImageDataGenerator().flow_from_directory(path+\"valid\", target_size=target_size, batch_size=batch_size,\n",
    "                                                   class_mode='categorical', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.\n",
    "\n",
    "def get_additional_layers(conv_layers):\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,2)),\n",
    "        Convolution2D(7,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "#         GlobalMaxPooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1282: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1282: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py:1282: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th\n",
      "(None, 3, 450, 450) (None, 7)\n"
     ]
    }
   ],
   "source": [
    "model = Vgg16BN(target_size).model #(360, 640)\n",
    "model.pop()\n",
    "\n",
    "for layer in model.layers: #Freeze feature layers\n",
    "    layer.trainable = False\n",
    "    \n",
    "    \n",
    "    \n",
    "conv_layers,_ = split_at(model, Convolution2D)\n",
    "\n",
    "for l in get_additional_layers(conv_layers):\n",
    "    model.add(l)\n",
    "    \n",
    "print (model.input_shape, model.output_shape)\n",
    "model.compile(Adam(lr=0.001), 'categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(get_additional_layers(conv_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 387s - loss: 0.8304 - acc: 0.7229 - val_loss: 1.4179 - val_acc: 0.7125\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 386s - loss: 0.3997 - acc: 0.8685 - val_loss: 0.5869 - val_acc: 0.8550\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 385s - loss: 0.2542 - acc: 0.9076 - val_loss: 0.4036 - val_acc: 0.9000\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 386s - loss: 0.1808 - acc: 0.9382 - val_loss: 0.5357 - val_acc: 0.8525\n",
      "Epoch 5/5\n",
      "2912/2912 [==============================] - 385s - loss: 0.1245 - acc: 0.9557 - val_loss: 0.5183 - val_acc: 0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3b39123810>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.save_weights(path+\"models/aug_cropped_1e3_5epch_val_acc_0.88.hd5\")\n",
    "model.load_weights(path+\"models/aug_cropped_1e3_5epch_val_acc_0.88.hd5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 392s - loss: 0.0793 - acc: 0.9756 - val_loss: 0.1645 - val_acc: 0.9550\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 395s - loss: 0.0367 - acc: 0.9914 - val_loss: 0.1575 - val_acc: 0.9600\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 396s - loss: 0.0294 - acc: 0.9924 - val_loss: 0.1522 - val_acc: 0.9625\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 396s - loss: 0.0168 - acc: 0.9979 - val_loss: 0.1522 - val_acc: 0.9575\n",
      "Epoch 5/5\n",
      "2912/2912 [==============================] - 396s - loss: 0.0139 - acc: 0.9979 - val_loss: 0.1453 - val_acc: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4737918990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save_weights(path+\"models/aug_cropped_1e4_5epch_val_acc_0.9650.hd5\")\n",
    "model.load_weights(path+\"models/aug_cropped_1e4_5epch_val_acc_0.9650.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 393s - loss: 0.0199 - acc: 0.9948 - val_loss: 0.1406 - val_acc: 0.9650\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 395s - loss: 0.0196 - acc: 0.9952 - val_loss: 0.1389 - val_acc: 0.9650\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 396s - loss: 0.0149 - acc: 0.9955 - val_loss: 0.1380 - val_acc: 0.9650\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 395s - loss: 0.0184 - acc: 0.9966 - val_loss: 0.1382 - val_acc: 0.9650\n",
      "Epoch 5/5\n",
      " 288/2912 [=>............................] - ETA: 313s - loss: 0.0100 - acc: 0.9965"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-44064f87e06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n\u001b[0;32m----> 3\u001b[0;31m                                  validation_data=val_flow, nb_val_samples=val_flow.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-5\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model is overfitting without any changes in validation accuracy, rolling back to previous weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making augmentation heavier (not done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.1, zoom_range=0.1,\n",
    "                                      channel_shift_range=5, height_shift_range=0.1, shear_range=0.1,\n",
    "                                      fill_mode='nearest',horizontal_flip=True)\n",
    "\n",
    "train_flow = gen.flow_from_directory(path+\"train\", target_size=target_size,\n",
    "            class_mode=\"categorical\", shuffle=True, batch_size=batch_size)\n",
    "\n",
    "val_flow = image.ImageDataGenerator().flow_from_directory(path+\"valid\", target_size=target_size, batch_size=batch_size,\n",
    "                                                   class_mode='categorical', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 392s - loss: 0.0325 - acc: 0.9883 - val_loss: 0.1878 - val_acc: 0.9525\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 395s - loss: 0.0350 - acc: 0.9876 - val_loss: 0.1756 - val_acc: 0.9575\n",
      "Epoch 3/5\n",
      " 288/2912 [=>............................] - ETA: 314s - loss: 0.0166 - acc: 0.9965"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-44064f87e06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n\u001b[0;32m----> 3\u001b[0;31m                                  validation_data=val_flow, nb_val_samples=val_flow.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's making the results worse, skipping for now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the original top 2 layers trainable\n",
    "ToDO\n",
    "After getting good enough accuracy on default VGG weights, it's time to allow the two top layers of original VGG to adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.ZeroPadding2D at 0x7ff26929c410>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7ff2692495d0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7ff269270c50>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7ff2692004d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7ff25e496f90>,\n",
       " <keras.layers.convolutional.Convolution2D at 0x7ff25e496f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7ff25e496750>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7ff25e496810>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-18:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers[-18:-14]:\n",
    "    layer.trainable = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2912/2912 [==============================] - 363s - loss: 0.0273 - acc: 0.9924 - val_loss: 0.1497 - val_acc: 0.9650\n",
      "Epoch 2/3\n",
      "2912/2912 [==============================] - 364s - loss: 0.0194 - acc: 0.9952 - val_loss: 0.1495 - val_acc: 0.9675\n",
      "Epoch 3/3\n",
      "2912/2912 [==============================] - 364s - loss: 0.0207 - acc: 0.9931 - val_loss: 0.1512 - val_acc: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff259118510>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = path+\"models/\" + 'fcn_aug_vggtunetop2_{val_acc:.2f}val_acc-{val_loss:.2f}-loss_{epoch}epoch.h5' \n",
    "ckpt = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "model.optimizer.lr = 1e-5\n",
    "\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=3,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample, callbacks = [ckpt,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best of default vgg:  loss: 0.0139 - acc: 0.9979 - val_loss: 0.1453 - val_acc: 0.9650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 363s - loss: 0.0375 - acc: 0.9883 - val_loss: 0.1560 - val_acc: 0.9650\n",
      "Epoch 2/5\n",
      "2912/2912 [==============================] - 364s - loss: 0.0340 - acc: 0.9883 - val_loss: 0.1543 - val_acc: 0.9675\n",
      "Epoch 3/5\n",
      "2912/2912 [==============================] - 364s - loss: 0.0267 - acc: 0.9945 - val_loss: 0.1560 - val_acc: 0.9650\n",
      "Epoch 4/5\n",
      "2912/2912 [==============================] - 364s - loss: 0.0366 - acc: 0.9876 - val_loss: 0.1592 - val_acc: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2443d9550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-6\n",
    "\n",
    "model.fit_generator(train_flow, samples_per_epoch=train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=val_flow, nb_val_samples=val_flow.nb_sample, callbacks = [ckpt,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "finished predicting round 0\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished predicting round 1\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished predicting round 2\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished predicting round 3\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished predicting round 4\n"
     ]
    }
   ],
   "source": [
    "test_datagen = gen\n",
    "num_iterations = 5\n",
    "preds = np.zeros((1000, 7)) #1000 test images, 7 fish types \n",
    "\n",
    "for i in range(num_iterations):\n",
    "    test_gen = test_datagen.flow_from_directory(path+\"test\", target_size=target_size, batch_size=batch_size,\n",
    "                                                        class_mode=None, shuffle=False)\n",
    "    preds += model.predict_generator(test_gen, val_samples=1000)\n",
    "    print (\"finished predicting round {}\".format(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n",
      "finished predicting round 0\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished predicting round 1\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished predicting round 2\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished predicting round 3\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished predicting round 4\n"
     ]
    }
   ],
   "source": [
    "test_datagen = gen\n",
    "num_iterations = 5\n",
    "preds = np.zeros((len(test_filenames), 7)) #1000 test images, 7 fish types \n",
    "\n",
    "for i in range(num_iterations):\n",
    "    test_gen = test_datagen.flow_from_directory(path+\"test\", target_size=target_size, batch_size=batch_size,\n",
    "                                                        class_mode=None, shuffle=False)\n",
    "    preds += model.predict_generator(test_gen, val_samples=len(test_filenames))\n",
    "    save_array(path+\"results/preds_light_aug_cropped_v1_part{}.dat\".format(i+1),preds)\n",
    "    \n",
    "    print (\"finished predicting round {}\".format(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.7026e+00,   1.6729e-04,   4.6911e-03,   2.0469e-04,   7.4383e-01,   1.5433e-04,\n",
       "          5.4837e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stage 1 preds[0:1]:\n",
    "\n",
    "array([[  4.9990e+00,   1.4059e-04,   4.9461e-06,   3.9640e-06,   7.3655e-04,   1.4233e-04,\n",
    "          7.2796e-06]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averaged_preds = preds / num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_array(path+\"results/preds_light_aug_cropped_v1.dat\",averaged_preds)\n",
    "save_array(path+\"results/stg2_preds_light_aug_cropped_v1.dat\",averaged_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Resnet (not done)\n",
    "Having problems with loss values, skipping for now in favor of a simple model with dense layers on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resnet50 import Resnet50\n",
    "\n",
    "resnet = Resnet50()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "resmodel = resnet.model \n",
    "\n",
    "# removing top layer that was designed for imagenet (1000 classes)\n",
    "resmodel.layers.pop()\n",
    "\n",
    "for layer in resmodel.layers: \n",
    "    layer.trainable=False\n",
    "\n",
    "\n",
    "# classifying 7 types of fish \n",
    "m = Dense(7, activation='softmax')(resmodel.layers[-1].output)\n",
    "resmodel= Model(resmodel.input, m)\n",
    "\n",
    "resmodel.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# RMSprop(lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n",
      "Epoch 1/5\n",
      "2912/2912 [==============================] - 81s - loss: nan - acc: 0.5251 - val_loss: nan - val_acc: 0.4750\n",
      "Epoch 2/5\n",
      "1312/2912 [============>.................] - ETA: 40s - loss: nan - acc: 0.5244"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9cc71cf56c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m resmodel.fit_generator(res_train_flow, samples_per_epoch=res_train_flow.nb_sample, nb_epoch=5,\n\u001b[0;32m---> 12\u001b[0;31m                                  validation_data=res_val_flow, nb_val_samples=res_val_flow.nb_sample)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1442\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating new flow generatros with size (224,224)\n",
    "res_train_flow = gen.flow_from_directory(path+\"train\", target_size=(224,224),\n",
    "            class_mode=\"categorical\", shuffle=True, batch_size=batch_size)\n",
    "\n",
    "res_val_flow = image.ImageDataGenerator().flow_from_directory(path+\"valid\", target_size=(224,224), batch_size=batch_size,\n",
    "                                                   class_mode='categorical', shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resmodel.fit_generator(res_train_flow, samples_per_epoch=res_train_flow.nb_sample, nb_epoch=5,\n",
    "                                 validation_data=res_val_flow, nb_val_samples=res_val_flow.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i get NaN for loss, skipping for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG with Dense layer on top\n",
    "\n",
    "Super mild augmentation,\n",
    "0.04 improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16bn import Vgg16BN\n",
    "\n",
    "# vgg_dense = Vgg16BN(size=target_size).model\n",
    "\n",
    "# vgg_dense.pop()\n",
    "# print (vgg_dense.input_shape, vgg_dense.output_shape)\n",
    "\n",
    "# vgg640.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# conv_layers,fc_layers = split_at(vgg640, Convolution2D)\n",
    "# conv_model = Sequential(conv_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        BatchNormalization(axis=1),\n",
    "        Dropout(p/3),\n",
    "        Flatten(),\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(7, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = image.ImageDataGenerator(\n",
    "        shear_range=0.05,\n",
    "        zoom_range=0.05,\n",
    "        rotation_range=7,\n",
    "        width_shift_range=0.07,\n",
    "        height_shift_range=0.07,\n",
    "    channel_shift_range=5,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "# set shuffle=False if pre-computing\n",
    "\n",
    "# aug_trn_batches = train_datagen.flow_from_directory(path+'train', target_size=target_size,\n",
    "#            class_mode='categorical', shuffle=False, batch_size=batch_size*2)\n",
    "\n",
    "\n",
    "# aug_val_batches = image.ImageDataGenerator().flow_from_directory(path+\"valid\", target_size=target_size, batch_size=batch_size*2,\n",
    "#                                                    class_mode='categorical', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#precomputing for faster computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 450, 450) (None, 512, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "vgg_dense = Vgg16BN(size=target_size).model\n",
    "\n",
    "vgg_dense.pop()\n",
    "print (vgg_dense.input_shape, vgg_dense.output_shape)\n",
    "\n",
    "\n",
    "conv_layers,fc_layers = split_at(vgg_dense, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "da_conv_featx2 = vgg_dense.predict_generator(aug_trn_batches, aug_trn_batches.nb_sample*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5824, 512, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_conv_featx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+\"results/da_conv_featx2.dat\",da_conv_featx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_trn_feat = load_array(path+'results/conv_trn_450.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2912, 512, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_trn_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = np.concatenate([conv_trn_feat, da_conv_featx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_trn_labels = np.concatenate([trn_labels]*3)\n",
    "conv_val_feat = load_array(path+'results/conv_val_450.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# memory cleaning\n",
    "# del conv_val_feat, da_conv_feat,conv_trn_feat,da_conv_featx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_model = Sequential(get_bn_layers(0.6))\n",
    "dense_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8736 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "8736/8736 [==============================] - 33s - loss: 0.8483 - acc: 0.8156 - val_loss: 0.5322 - val_acc: 0.9250\n",
      "Epoch 2/5\n",
      "8736/8736 [==============================] - 33s - loss: 0.0996 - acc: 0.9752 - val_loss: 0.3817 - val_acc: 0.9575\n",
      "Epoch 3/5\n",
      "8736/8736 [==============================] - 33s - loss: 0.0419 - acc: 0.9899 - val_loss: 0.3930 - val_acc: 0.9625\n",
      "Epoch 4/5\n",
      "8736/8736 [==============================] - 33s - loss: 0.0353 - acc: 0.9920 - val_loss: 0.3403 - val_acc: 0.9675\n",
      "Epoch 5/5\n",
      "8736/8736 [==============================] - 33s - loss: 0.0326 - acc: 0.9930 - val_loss: 0.3397 - val_acc: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8078cf8c50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.fit(da_conv_feat, da_trn_labels, batch_size=64, nb_epoch=5, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8736 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "8736/8736 [==============================] - 33s - loss: 0.0347 - acc: 0.9910 - val_loss: 0.3026 - val_acc: 0.9625\n",
      "Epoch 2/3\n",
      "8736/8736 [==============================] - 33s - loss: 0.0200 - acc: 0.9938 - val_loss: 0.2869 - val_acc: 0.9650\n",
      "Epoch 3/3\n",
      "8736/8736 [==============================] - 33s - loss: 0.0382 - acc: 0.9904 - val_loss: 0.3190 - val_acc: 0.9525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80844bac10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.optimizer.lr = 1e-5\n",
    "dense_model.fit(da_conv_feat, da_trn_labels, batch_size=64, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8736 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "8736/8736 [==============================] - 34s - loss: 0.0237 - acc: 0.9942 - val_loss: 0.3196 - val_acc: 0.9675\n",
      "Epoch 2/3\n",
      "8736/8736 [==============================] - 33s - loss: 0.0294 - acc: 0.9936 - val_loss: 0.2976 - val_acc: 0.9625\n",
      "Epoch 3/3\n",
      "8736/8736 [==============================] - 33s - loss: 0.0234 - acc: 0.9937 - val_loss: 0.2968 - val_acc: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8078cf7b90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.optimizer.lr = 1e-7\n",
    "dense_model.fit(da_conv_feat, da_trn_labels, batch_size=64, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dense_model.save_weights(path+\"models/dense_precomputed_aug_1e7_val_acc_0.9625.h5\")\n",
    "\n",
    "dense_model.load_weights(path+\"models/dense_precomputed_aug_1e7_val_acc_0.9625.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Making predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "finished prediction round  1\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished prediction round  2\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished prediction round  3\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished prediction round  4\n",
      "Found 1000 images belonging to 1 classes.\n",
      "finished prediction round  5\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros(shape=(1000, 7))\n",
    "\n",
    "nb_aug = 5\n",
    "for aug in range(nb_aug):\n",
    "    \n",
    "    test_batches = train_datagen.flow_from_directory(path+\"test\", target_size=target_size,\n",
    "                class_mode=None, shuffle=False, batch_size=32)\n",
    "\n",
    "\n",
    "    conv_test_feat = vgg_dense.predict_generator(test_batches, 1000)\n",
    "    predictions += dense_model.predict(conv_test_feat, batch_size=32)\n",
    "    \n",
    "    print (\"finished prediction round \", aug+1)\n",
    "predictions /= nb_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+\"results/preds_dense_precomputed_aug_1e7_val_acc_0.9625.dat\",predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 2 predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12153 images belonging to 1 classes.\n",
      "finished prediction round  1\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished prediction round  2\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished prediction round  3\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished prediction round  4\n",
      "Found 12153 images belonging to 1 classes.\n",
      "finished prediction round  5\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros(shape=(len(test_filenames), 7))\n",
    "\n",
    "nb_aug = 5\n",
    "for aug in range(nb_aug):\n",
    "    \n",
    "    test_batches = train_datagen.flow_from_directory(path+\"test\", target_size=target_size,\n",
    "                class_mode=None, shuffle=False, batch_size=32)\n",
    "\n",
    "\n",
    "    conv_test_feat = vgg_dense.predict_generator(test_batches, len(test_filenames))\n",
    "    predictions += dense_model.predict(conv_test_feat, batch_size=32)\n",
    "    \n",
    "    print (\"finished prediction round \", aug+1)\n",
    "    \n",
    "    save_array(path+\"results/stg2_croppreds_dense_precomputed_aug_part{}.dat\".format(aug+1),predictions)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "predictions /= nb_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+\"results/stg2_preds_dense_precomputed_aug_1e7_val_acc_0.9625.dat\",predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12153, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.7109e-01,   9.4312e-04,   5.9165e-03,   9.3670e-03,   1.9749e-01,   3.4832e-04,\n",
       "          5.1485e-01],\n",
       "       [  9.8965e-01,   7.4218e-05,   9.4611e-05,   4.9753e-06,   1.0168e-02,   3.6449e-07,\n",
       "          5.6005e-06]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla VGG with Dense layer on top\n",
    "\n",
    "vanilla: no aug, precomputed \n",
    "Result: worse by 0.05 points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2912 images belonging to 7 classes.\n",
      "Found 400 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train', target_size)\n",
    "val = get_data(path+'valid', target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/trn_450.dat', trn)\n",
    "save_array(path+'results/trn_450.dat', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 450, 450) (None, 512, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "vgg450 = Vgg16BN(target_size).model\n",
    "vgg450.pop()\n",
    "print (vgg450.input_shape, vgg450.output_shape)\n",
    "vgg450.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 42s    \n",
      "2912/2912 [==============================] - 314s   \n"
     ]
    }
   ],
   "source": [
    "conv_val_feat = vgg450.predict(val, batch_size=32, verbose=1)\n",
    "conv_trn_feat = vgg450.predict(trn, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_450.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_trn_450.dat', conv_trn_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "1000/1000 [==============================] - 107s   \n"
     ]
    }
   ],
   "source": [
    "test = get_data(path+'test', target_size)\n",
    "save_array(path+'results/test_450.dat', test)\n",
    "\n",
    "conv_test_feat = vgg450.predict(test, batch_size=32, verbose=1)\n",
    "save_array(path+'results/conv_test_450.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layers,fc_layers = split_at(vgg450, Convolution2D)\n",
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2912 samples, validate on 400 samples\n",
      "Epoch 1/2\n",
      "2912/2912 [==============================] - 13s - loss: 0.7027 - acc: 0.8156 - val_loss: 0.2767 - val_acc: 0.9375\n",
      "Epoch 2/2\n",
      "2912/2912 [==============================] - 13s - loss: 0.0685 - acc: 0.9787 - val_loss: 0.2963 - val_acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febe1eeeed0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bn_model.save_weights(path+\"models/vanilla_vgg_precompute_1e3_0.9475.h5\")\n",
    "bn_model.load_weights(path+\"models/vanilla_vgg_precompute_1e3_0.9475.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2912 samples, validate on 400 samples\n",
      "Epoch 1/3\n",
      "2912/2912 [==============================] - 13s - loss: 0.0549 - acc: 0.9866 - val_loss: 0.2333 - val_acc: 0.9500\n",
      "Epoch 2/3\n",
      "2912/2912 [==============================] - 13s - loss: 0.0404 - acc: 0.9870 - val_loss: 0.2538 - val_acc: 0.9450\n",
      "Epoch 3/3\n",
      "2912/2912 [==============================] - 13s - loss: 0.0554 - acc: 0.9849 - val_loss: 0.3063 - val_acc: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7febe1eef210>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.optimizer.lr = 1e-7\n",
    "bn_model.fit(conv_trn_feat, trn_labels, batch_size=batch_size, nb_epoch=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_vanilla = bn_model.predict(conv_test_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+\"results/preds_dense_vanilla_1e3.dat\",preds_vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classes = ['ALB', 'BET', 'DOL', 'LAG','NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "# nofish_prob = load_array(\"data/fishnofish/results/preds_640_loss_0.0244_acc_0.9913_val_loss_0.2086_val_acc_0.9778.dat\")\n",
    "# nofish_prob = load_array(\"data/fishnofish/results/pred_stg1_fullconv_aug_no_pseudo.dat\")\n",
    "# nofish_prob = load_array(\"data/fishnofish/results/final_stg1_weights_ensumble.dat\")\n",
    "\n",
    "nofish_prob = load_array(\"data/fishnofish/results/final_stg1_weights_ensumble_with_pseudo.dat\")\n",
    "\n",
    "\n",
    "\n",
    "nofish_prob = nofish_prob[:, 1] #nofish class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "preds = load_array(path+\"results/preds_light_aug_cropped_v1.dat\")\n",
    "# preds_vanilla = load_array(path+\"results/preds_dense_vanilla_1e3.dat\") >> made it worse by 0.7 point\n",
    "preds_denseaug = load_array(path+\"results/preds_dense_precomputed_aug_1e7_val_acc_0.9625.dat\")\n",
    "\n",
    "subm_name = path+'submit/preds_cropped_v1_denseaugprecompute.gz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds+= preds_denseaug\n",
    "preds /= 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_predictions(predictions):\n",
    "    \"\"\"Weights predictions based on probability image contains a fish as predicted by fish detector model\"\"\"\n",
    "    no_fish = predictions[:, 4]\n",
    "    fish = np.delete(predictions, 4, axis=1)\n",
    "\n",
    "    weights = -1. * (no_fish - 1.)\n",
    "    weights = weights.reshape(1000, 1)\n",
    "\n",
    "    fish = weights * fish\n",
    "    preds = np.insert(fish, 4, no_fish, axis=1)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with_nofish_probs = np.insert(preds, 4, nofish_prob, axis=1)\n",
    "weighted_preds = weight_predictions(with_nofish_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 0.85#0.82\n",
    "subm = np.clip(weighted_preds, (1-f)/7, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_06237.jpg</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_06893.jpg</td>\n",
       "      <td>0.299337</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.022149</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.638110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_02082.jpg</td>\n",
       "      <td>0.360107</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.582517</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.054423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_06261.jpg</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.107314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_03628.jpg</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "0  img_06237.jpg  0.850000  0.021429  0.021429  0.021429  0.021429  0.021429   \n",
       "1  img_06893.jpg  0.299337  0.021429  0.021429  0.021429  0.021429  0.022149   \n",
       "2  img_02082.jpg  0.360107  0.021429  0.021429  0.021429  0.582517  0.021429   \n",
       "3  img_06261.jpg  0.850000  0.021429  0.021429  0.021429  0.021429  0.021429   \n",
       "4  img_03628.jpg  0.850000  0.021429  0.021429  0.021429  0.021429  0.021429   \n",
       "\n",
       "      SHARK       YFT  \n",
       "0  0.021429  0.021429  \n",
       "1  0.021429  0.638110  \n",
       "2  0.021429  0.054423  \n",
       "3  0.021429  0.107314  \n",
       "4  0.021429  0.021429  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', raw_test_filenames)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/cropped_nof_excl/submit/preds_cropped_v1_denseaugprecompute.gz' target='_blank'>data/cropped_nof_excl/submit/preds_cropped_v1_denseaugprecompute.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/fish/data/cropped_nof_excl/submit/preds_cropped_v1_denseaugprecompute.gz"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')\n",
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
